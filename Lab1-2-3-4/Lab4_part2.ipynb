{"cells":[{"cell_type":"markdown","metadata":{"id":"nNmutQmT9lsv"},"source":["#  Neural Networks: Regression on House Pricing Dataset\n","We consider a reduced version of a dataset containing house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.\n","\n","https://www.kaggle.com/harlfoxem/housesalesprediction\n","\n","For each house we know 18 house features (e.g., number of bedrooms, number of bathrooms, etc.) plus its price, that is what we would like to predict."]},{"cell_type":"markdown","metadata":{"id":"Au-QD_aB9ls0"},"source":["## Insert your ID number (\"numero di matricola\") below"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"msn4WT2u9ls1","executionInfo":{"status":"ok","timestamp":1642527265530,"user_tz":-60,"elapsed":287,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}}},"outputs":[],"source":["# put here your ``numero di matricola''\n","numero_di_matricola = 1\n"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"4tAJtTrF9ls2","executionInfo":{"status":"ok","timestamp":1642527266351,"user_tz":-60,"elapsed":4,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}}},"outputs":[],"source":["# import all packages needed\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","%matplotlib inline\n"]},{"cell_type":"markdown","metadata":{"id":"sw0HoA8L9ls3"},"source":["Load the data, remove data samples/points with missing values (NaN) and take a look at them."]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"5chgsV-U9ls4","executionInfo":{"status":"ok","timestamp":1642527267141,"user_tz":-60,"elapsed":793,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}},"outputId":"34dd6ce6-9956-4ea3-c8d0-85fa2011250e"},"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-ed85ddff-ad96-428f-8cab-255b814e7b84\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>price</th>\n","      <th>bedrooms</th>\n","      <th>bathrooms</th>\n","      <th>sqft_living</th>\n","      <th>sqft_lot</th>\n","      <th>floors</th>\n","      <th>waterfront</th>\n","      <th>view</th>\n","      <th>condition</th>\n","      <th>grade</th>\n","      <th>sqft_above</th>\n","      <th>sqft_basement</th>\n","      <th>yr_built</th>\n","      <th>yr_renovated</th>\n","      <th>zipcode</th>\n","      <th>lat</th>\n","      <th>long</th>\n","      <th>sqft_living15</th>\n","      <th>sqft_lot15</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>2.161300e+04</td>\n","      <td>2.161300e+04</td>\n","      <td>21613.000000</td>\n","      <td>21613.000000</td>\n","      <td>21613.000000</td>\n","      <td>2.161300e+04</td>\n","      <td>21613.000000</td>\n","      <td>21613.000000</td>\n","      <td>21613.000000</td>\n","      <td>21613.000000</td>\n","      <td>21613.000000</td>\n","      <td>21613.000000</td>\n","      <td>21613.000000</td>\n","      <td>21613.000000</td>\n","      <td>21613.000000</td>\n","      <td>21613.000000</td>\n","      <td>21613.000000</td>\n","      <td>21613.000000</td>\n","      <td>21613.000000</td>\n","      <td>21613.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>4.580302e+09</td>\n","      <td>5.400881e+05</td>\n","      <td>3.370842</td>\n","      <td>2.114757</td>\n","      <td>2079.899736</td>\n","      <td>1.510697e+04</td>\n","      <td>1.494309</td>\n","      <td>0.007542</td>\n","      <td>0.234303</td>\n","      <td>3.409430</td>\n","      <td>7.656873</td>\n","      <td>1788.390691</td>\n","      <td>291.509045</td>\n","      <td>1971.005136</td>\n","      <td>84.402258</td>\n","      <td>98077.939805</td>\n","      <td>47.560053</td>\n","      <td>-122.213896</td>\n","      <td>1986.552492</td>\n","      <td>12768.455652</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>2.876566e+09</td>\n","      <td>3.671272e+05</td>\n","      <td>0.930062</td>\n","      <td>0.770163</td>\n","      <td>918.440897</td>\n","      <td>4.142051e+04</td>\n","      <td>0.539989</td>\n","      <td>0.086517</td>\n","      <td>0.766318</td>\n","      <td>0.650743</td>\n","      <td>1.175459</td>\n","      <td>828.090978</td>\n","      <td>442.575043</td>\n","      <td>29.373411</td>\n","      <td>401.679240</td>\n","      <td>53.505026</td>\n","      <td>0.138564</td>\n","      <td>0.140828</td>\n","      <td>685.391304</td>\n","      <td>27304.179631</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000102e+06</td>\n","      <td>7.500000e+04</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>290.000000</td>\n","      <td>5.200000e+02</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>290.000000</td>\n","      <td>0.000000</td>\n","      <td>1900.000000</td>\n","      <td>0.000000</td>\n","      <td>98001.000000</td>\n","      <td>47.155900</td>\n","      <td>-122.519000</td>\n","      <td>399.000000</td>\n","      <td>651.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>2.123049e+09</td>\n","      <td>3.219500e+05</td>\n","      <td>3.000000</td>\n","      <td>1.750000</td>\n","      <td>1427.000000</td>\n","      <td>5.040000e+03</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>3.000000</td>\n","      <td>7.000000</td>\n","      <td>1190.000000</td>\n","      <td>0.000000</td>\n","      <td>1951.000000</td>\n","      <td>0.000000</td>\n","      <td>98033.000000</td>\n","      <td>47.471000</td>\n","      <td>-122.328000</td>\n","      <td>1490.000000</td>\n","      <td>5100.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>3.904930e+09</td>\n","      <td>4.500000e+05</td>\n","      <td>3.000000</td>\n","      <td>2.250000</td>\n","      <td>1910.000000</td>\n","      <td>7.618000e+03</td>\n","      <td>1.500000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>3.000000</td>\n","      <td>7.000000</td>\n","      <td>1560.000000</td>\n","      <td>0.000000</td>\n","      <td>1975.000000</td>\n","      <td>0.000000</td>\n","      <td>98065.000000</td>\n","      <td>47.571800</td>\n","      <td>-122.230000</td>\n","      <td>1840.000000</td>\n","      <td>7620.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>7.308900e+09</td>\n","      <td>6.450000e+05</td>\n","      <td>4.000000</td>\n","      <td>2.500000</td>\n","      <td>2550.000000</td>\n","      <td>1.068800e+04</td>\n","      <td>2.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>4.000000</td>\n","      <td>8.000000</td>\n","      <td>2210.000000</td>\n","      <td>560.000000</td>\n","      <td>1997.000000</td>\n","      <td>0.000000</td>\n","      <td>98118.000000</td>\n","      <td>47.678000</td>\n","      <td>-122.125000</td>\n","      <td>2360.000000</td>\n","      <td>10083.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>9.900000e+09</td>\n","      <td>7.700000e+06</td>\n","      <td>33.000000</td>\n","      <td>8.000000</td>\n","      <td>13540.000000</td>\n","      <td>1.651359e+06</td>\n","      <td>3.500000</td>\n","      <td>1.000000</td>\n","      <td>4.000000</td>\n","      <td>5.000000</td>\n","      <td>13.000000</td>\n","      <td>9410.000000</td>\n","      <td>4820.000000</td>\n","      <td>2015.000000</td>\n","      <td>2015.000000</td>\n","      <td>98199.000000</td>\n","      <td>47.777600</td>\n","      <td>-121.315000</td>\n","      <td>6210.000000</td>\n","      <td>871200.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed85ddff-ad96-428f-8cab-255b814e7b84')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ed85ddff-ad96-428f-8cab-255b814e7b84 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ed85ddff-ad96-428f-8cab-255b814e7b84');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                 id         price  ...  sqft_living15     sqft_lot15\n","count  2.161300e+04  2.161300e+04  ...   21613.000000   21613.000000\n","mean   4.580302e+09  5.400881e+05  ...    1986.552492   12768.455652\n","std    2.876566e+09  3.671272e+05  ...     685.391304   27304.179631\n","min    1.000102e+06  7.500000e+04  ...     399.000000     651.000000\n","25%    2.123049e+09  3.219500e+05  ...    1490.000000    5100.000000\n","50%    3.904930e+09  4.500000e+05  ...    1840.000000    7620.000000\n","75%    7.308900e+09  6.450000e+05  ...    2360.000000   10083.000000\n","max    9.900000e+09  7.700000e+06  ...    6210.000000  871200.000000\n","\n","[8 rows x 20 columns]"]},"metadata":{},"execution_count":48}],"source":["# load the data\n","df = pd.read_csv('kc_house_data.csv', sep=',')\n","\n","# remove the data samples with missing values (NaN)\n","df = df.dropna()\n","\n","df.describe()\n"]},{"cell_type":"markdown","metadata":{"id":"GMjX5pMg9ls6"},"source":["Extract input and output data. We want to predict the price by using features other than id as input."]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E3F8Xcit9ls7","executionInfo":{"status":"ok","timestamp":1642527267141,"user_tz":-60,"elapsed":16,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}},"outputId":"871f8b44-9614-48fb-e3a3-085ac7e48675"},"outputs":[{"output_type":"stream","name":"stdout","text":["Amount of data: 21613\n"]}],"source":["Data = df.values\n","# m = number of input samples\n","m = Data.shape[0]\n","print(\"Amount of data:\", m)\n","Y = Data[:m, 2]\n","X = Data[:m, 3:]\n"]},{"cell_type":"markdown","metadata":{"id":"ZyKWt2aT9ls8"},"source":["## Data Pre-Processing\n","\n","We split the data into 3 parts: one will be used for training and choosing the parameters, one for choosing among different models, and one for testing. The part for training and choosing the parameters will consist of $2/3$ of all samples, the one for choosing among different models will consist of $1/6$ of all samples, while the other part consists of the remaining $1/6$-th of all samples."]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MoQyRiQn9ls9","executionInfo":{"status":"ok","timestamp":1642527267142,"user_tz":-60,"elapsed":14,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}},"outputId":"75c2b2ff-7bc8-4072-f913-8f6f85f0dffd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Amount of data for training and deciding parameters: 14408\n","Amount of data for validation (choosing among different models): 3602\n","Amount of data for test: 3603\n"]}],"source":["# Split data into train (2/3 of samples), validation (1/6 of samples), and test data (the rest)\n","from sklearn.model_selection import train_test_split\n","m_train = int(2./3.*m)\n","m_val = int((m-m_train)/2.)\n","m_test = m - m_train - m_val\n","print(\"Amount of data for training and deciding parameters:\", m_train)\n","print(\"Amount of data for validation (choosing among different models):\", m_val)\n","print(\"Amount of data for test:\", m_test)\n","\n","Xtrain_and_val, Xtest, Ytrain_and_val, Ytest = train_test_split(\n","    X, Y, test_size=m_test/m, random_state=numero_di_matricola)\n","Xtrain, Xval, Ytrain, Yval = train_test_split(\n","    Xtrain_and_val, Ytrain_and_val, test_size=m_val/(m_train+m_val), random_state=numero_di_matricola)\n"]},{"cell_type":"markdown","metadata":{"id":"KAjngGD69ls-"},"source":["Let's standardize the data."]},{"cell_type":"code","execution_count":51,"metadata":{"id":"xYHD4YW79ls-","executionInfo":{"status":"ok","timestamp":1642527267142,"user_tz":-60,"elapsed":13,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}}},"outputs":[],"source":["# Data pre-processing\n","from sklearn import preprocessing\n","scaler = preprocessing.StandardScaler().fit(Xtrain)\n","Xtrain_scaled = scaler.transform(Xtrain)\n","Xtrain_and_val_scaled = scaler.transform(Xtrain_and_val)\n","Xval_scaled = scaler.transform(Xval)\n","Xtest_scaled = scaler.transform(Xtest)\n"]},{"cell_type":"markdown","metadata":{"id":"0MAEXrVn9ls_"},"source":["## Neural Networks\n","Let's start by learning a simple neural network with 1 hidden node.\n","Note: we are going to use the input parameter solver='lbfgs' and random_state=numero_di_matricola to fix the random seed (so results are reproducible)."]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XNZSoz2F9ls_","executionInfo":{"status":"ok","timestamp":1642527267143,"user_tz":-60,"elapsed":13,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}},"outputId":"9082bae4-4802-48b3-c587-20831e970940"},"outputs":[{"output_type":"stream","name":"stdout","text":["training error 0.29559863217335836\n","validation error 0.31059586274997\n","coeffs (weights from input layer to the hidden neuron):\n"," [array([[ -729.87486122],\n","       [  831.2303795 ],\n","       [ 1799.75148514],\n","       [  124.60013752],\n","       [  119.38146152],\n","       [ 1146.72653943],\n","       [  929.24974225],\n","       [  381.66546344],\n","       [ 2763.39475395],\n","       [ 1668.15762936],\n","       [  601.24288021],\n","       [-1820.67409758],\n","       [  126.75146032],\n","       [ -703.0951815 ],\n","       [ 1943.23571365],\n","       [ -759.15710926],\n","       [  300.88988146],\n","       [ -202.38632292]]), array([[43.04451409]])]\n","intercepts (weights of biases of input and hidden layers):\n"," [array([12602.10199491]), array([-5370.82907044])]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"]}],"source":["# let's load the MLPRegressor\n","from sklearn.neural_network import MLPRegressor\n","\n","# let's define the model\n","# the default model performs extremely bad : R^2 negative...\n","# so we use the LBFGS solver\n","NN = MLPRegressor(hidden_layer_sizes=(1,), random_state=numero_di_matricola, solver=\"lbfgs\")\n","\n","# let's learn the model on training data\n","NN.fit(Xtrain_scaled, Ytrain)\n","\n","# let's print the error (1 - R^2) on training data\n","print(\"training error\", 1-NN.score(Xtrain_scaled, Ytrain))\n","\n","# let's print the error (1 - R^2) on validation data\n","print(\"validation error\", 1-NN.score(Xval_scaled, Yval))\n","\n","# let's print the coefficients of the model for the input nodes (but not the bias)\n","print(\"coeffs (weights from input layer to the hidden neuron):\\n\", NN.coefs_)\n","\n","# let's print the coefficient for the bias (i.e., the bias)\n","print(\"intercepts (weights of biases of input and hidden layers):\\n\", NN.intercepts_)\n"]},{"cell_type":"markdown","metadata":{"id":"DYDc5vVa9ltA"},"source":["## Neural Networks vs Linear Models\n","\n","Let's learn a linear model on the other same data and compare the results with the simple NN above."]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jjZ801WZ9ltA","executionInfo":{"status":"ok","timestamp":1642527267143,"user_tz":-60,"elapsed":11,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}},"outputId":"b14c0bb5-a881-4830-a70e-c10963f69eea"},"outputs":[{"output_type":"stream","name":"stdout","text":["training error 0.2971718751921173\n","validation error 0.3113977005845858\n","coef:\n"," [-32024.37205507  30857.38702503  81520.08274701   4358.71015115\n","   4210.76613832  52271.26763491  39275.36300358  17128.54179271\n"," 114582.56984907  76295.51474423  25805.04611065 -75175.13990886\n","   6580.79073948 -29421.4439202   84347.82703947 -28791.00545764\n","   9512.22430408  -8875.3212928 ]\n","intercept:\n"," 539147.4730705542\n"]}],"source":["from sklearn import linear_model\n","\n","LR = linear_model.LinearRegression()\n","\n","LR.fit(Xtrain_scaled, Ytrain)\n","\n","# let's print the error (1 - R^2) on training data\n","print(\"training error\", 1-LR.score(Xtrain_scaled, Ytrain))\n","\n","# let's print the error (1 - R^2) on validation data\n","print(\"validation error\", 1-LR.score(Xval_scaled, Yval))\n","\n","# let's print the coefficients of the model for the input nodes (but not the bias)\n","print(\"coef:\\n\", LR.coef_)\n","\n","# let's print the coefficient for the bias (i.e., the bias)\n","print(\"intercept:\\n\", LR.intercept_)"]},{"cell_type":"markdown","metadata":{"id":"8I8N5oiT9ltB"},"source":["Is there a way to make a NN network learn a linear model?\n","\n","Let's first check what is the loss used by MLPRegressor..."]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i9RE2Tt89ltB","executionInfo":{"status":"ok","timestamp":1642527267144,"user_tz":-60,"elapsed":10,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}},"outputId":"fe950e9e-6aa5-42a7-d354-6c63fff15771"},"outputs":[{"output_type":"stream","name":"stdout","text":["training error 0.2971718758498938\n","validation error 0.3113975786278589\n","coeffs (weights from input layer to the hidden neuron):\n"," [array([[  53.05892257],\n","       [ -51.11646483],\n","       [-135.33248854],\n","       [  -7.22545107],\n","       [  -6.98193883],\n","       [ -86.59959823],\n","       [ -65.07184335],\n","       [ -28.37751298],\n","       [-189.83995551],\n","       [-126.15950406],\n","       [ -42.62168198],\n","       [ 124.55003826],\n","       [ -10.9016803 ],\n","       [  48.75330118],\n","       [-139.74756357],\n","       [  47.70252967],\n","       [ -15.76187867],\n","       [  14.7062653 ]]), array([[-603.573508]])]\n","intercepts (weights of biases of input and hidden layers):\n"," [array([-892.60243245]), array([402.44197475])]\n"]}],"source":["# let's write the code to learn a linear model with NN: how?\n","\n","# let's define the model\n","# (notice, by default MLPRegressor uses squared loss, exactly like the linear model regressor)\n","NN = MLPRegressor(hidden_layer_sizes=(1,), random_state=numero_di_matricola, solver=\"lbfgs\", activation=\"identity\")\n","\n","# let's learn the model on training data\n","NN.fit(Xtrain_scaled, Ytrain)\n","\n","# let's print the error (1 - R^2) on training data\n","print(\"training error\", 1-NN.score(Xtrain_scaled, Ytrain))\n","\n","# let's print the error (1 - R^2) on validation data\n","print(\"validation error\", 1-NN.score(Xval_scaled, Yval))\n","\n","# let's print the coefficients of the model for the input nodes (but not the bias)\n","print(\"coeffs (weights from input layer to the hidden neuron):\\n\", NN.coefs_)\n","\n","# let's print the coefficient for the bias (i.e., the bias)\n","print(\"intercepts (weights of biases of input and hidden layers):\\n\", NN.intercepts_)\n","\n","# notice intercepts[0]*coefs[1,0]+intercepts[1] = intercept or linear model regressor"]},{"cell_type":"markdown","metadata":{"id":"KxhgzFNA9ltB"},"source":["Note that there is an $\\ell_2$ regularization term in MLPRegressor. What about making it smaller?"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"1bpzBr0U9ltC","executionInfo":{"status":"ok","timestamp":1642527267144,"user_tz":-60,"elapsed":9,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}}},"outputs":[],"source":["# COMPLETE\n"]},{"cell_type":"markdown","metadata":{"id":"F0S-vd869ltC"},"source":["## More Complex NNs\n","\n","Let's try more complex NN, for example increasing the number of nodes in the only hidden layer, or increasing the number of hidden layers."]},{"cell_type":"markdown","metadata":{"id":"MdjAFJUd9ltC"},"source":["Let's build a NN with 2 nodes in the only hidden layer"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N3LP-4o59ltC","executionInfo":{"status":"ok","timestamp":1642527268041,"user_tz":-60,"elapsed":905,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}},"outputId":"01885507-a2e8-49dd-cbb5-017be549fb5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["training error 0.2153208979874328\n","validation error 0.21690093519558962\n","coeffs (weights from input layer to the hidden neuron):\n"," [array([[ -32.10729555,  -16.38981375],\n","       [ 116.45979582,   27.15117946],\n","       [ 171.25183209,   55.99693335],\n","       [-288.5583707 ,   17.21660766],\n","       [ -98.12489751,   28.17114291],\n","       [ 106.85686761,   14.07110269],\n","       [ -29.1029425 ,   52.61406787],\n","       [  61.74518617,   26.7848754 ],\n","       [ 277.04724345,  136.12529807],\n","       [ 181.99576479,   49.44585152],\n","       [  13.99059296,   24.08211091],\n","       [ -53.63301901,  -90.12891609],\n","       [  28.99586503,    9.77813247],\n","       [-198.64003826,  -28.10630235],\n","       [ 296.80982024,  120.20746084],\n","       [-454.94082664,  -18.0556324 ],\n","       [  12.28997402,   62.54149262],\n","       [-320.04383237,   -5.6423691 ]]), array([[748.23410896],\n","       [656.7949309 ]])]\n","intercepts (weights of biases of input and hidden layers):\n"," [array([-1084.99886264,   778.08863079]), array([782.89383154])]\n"]}],"source":["# let's build a NN with 2 nodes in the only hidden layer\n","\n","# let's define the model\n","NN = MLPRegressor(hidden_layer_sizes=(2,), random_state=numero_di_matricola, solver=\"lbfgs\")\n","\n","# let's learn the model on training data\n","NN.fit(Xtrain_scaled, Ytrain)\n","\n","# let's print the error (1 - R^2) on training data\n","print(\"training error\", 1-NN.score(Xtrain_scaled, Ytrain))\n","\n","# let's print the error (1 - R^2) on validation data\n","print(\"validation error\", 1-NN.score(Xval_scaled, Yval))\n","\n","# let's print the coefficients of the model for the input nodes (but not the bias)\n","print(\"coeffs (weights from input layer to the hidden neuron):\\n\", NN.coefs_)\n","\n","# let's print the coefficient for the bias (i.e., the bias)\n","print(\"intercepts (weights of biases of input and hidden layers):\\n\", NN.intercepts_)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"X0bVwAY09ltD"},"source":["Let's build a NN with 5 nodes in the only hidden layer"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9E-5D2Zf9ltD","executionInfo":{"status":"ok","timestamp":1642527268896,"user_tz":-60,"elapsed":857,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}},"outputId":"b480de8d-072d-489d-b5a9-cdbba6324be7"},"outputs":[{"output_type":"stream","name":"stdout","text":["training error 0.16409610419710907\n","validation error 0.17530164058251185\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"]}],"source":["# let's build a NN with 5 nodes in the only hidden layer\n","\n","# let's define the model\n","NN = MLPRegressor(hidden_layer_sizes=(5,), random_state=numero_di_matricola, solver=\"lbfgs\")\n","\n","# let's learn the model on training data\n","NN.fit(Xtrain_scaled, Ytrain)\n","\n","# let's print the error (1 - R^2) on training data\n","print(\"training error\", 1-NN.score(Xtrain_scaled, Ytrain))\n","\n","# let's print the error (1 - R^2) on validation data\n","print(\"validation error\", 1-NN.score(Xval_scaled, Yval))\n","\n","# # let's print the coefficients of the model for the input nodes (but not the bias)\n","# print(\"coeffs (weights from input layer to the hidden neuron):\\n\", NN.coefs_)\n","\n","# # let's print the coefficient for the bias (i.e., the bias)\n","# print(\"intercepts (weights of biases of input and hidden layers):\\n\", NN.intercepts_)"]},{"cell_type":"markdown","metadata":{"id":"hY-oEDxh9ltE"},"source":["Note that with a smaller number of iterations we had a larger error on training set but a smaller error on validation data -> \"early stopping is a form of regularization\""]},{"cell_type":"markdown","metadata":{"id":"pwNwAR1_9ltE"},"source":["Let's build a NN with 10 nodes in the only hidden layer"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5m-dz5RG9ltE","executionInfo":{"status":"ok","timestamp":1642527270115,"user_tz":-60,"elapsed":1223,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}},"outputId":"b9271cd5-428b-45a1-b276-b62fbbfa70b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["training error 0.127399261512813\n","validation error 0.14331396707337007\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"]}],"source":["# let's build a NN with 10 nodes in the only hidden layer\n","\n","# let's define the model\n","NN = MLPRegressor(hidden_layer_sizes=(10,), random_state=numero_di_matricola, solver=\"lbfgs\")\n","\n","# let's learn the model on training data\n","NN.fit(Xtrain_scaled, Ytrain)\n","\n","# let's print the error (1 - R^2) on training data\n","print(\"training error\", 1-NN.score(Xtrain_scaled, Ytrain))\n","\n","# let's print the error (1 - R^2) on validation data\n","print(\"validation error\", 1-NN.score(Xval_scaled, Yval))\n","\n","# # let's print the coefficients of the model for the input nodes (but not the bias)\n","# print(\"coeffs (weights from input layer to the hidden neuron):\\n\", NN.coefs_)\n","\n","# # let's print the coefficient for the bias (i.e., the bias)\n","# print(\"intercepts (weights of biases of input and hidden layers):\\n\", NN.intercepts_)\n"]},{"cell_type":"markdown","metadata":{"id":"l9QgMVpO9ltE"},"source":["Let's build a NN with 100 nodes in the only hidden layer. Note that this is the default!"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TM9PBr1S9ltF","executionInfo":{"status":"ok","timestamp":1642527316607,"user_tz":-60,"elapsed":46495,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}},"outputId":"f13c69fa-90fb-4a34-bcbf-839577d77cfc"},"outputs":[{"output_type":"stream","name":"stdout","text":["training error 0.05750129141969351\n","validation error 0.11642854289390092\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"]}],"source":["# let's build a NN with 100 nodes in the only hidden layer\n","\n","# let's define the model\n","NN = MLPRegressor(max_iter=1000, hidden_layer_sizes=(100,), random_state=numero_di_matricola, solver=\"lbfgs\")\n","\n","# let's learn the model on training data\n","NN.fit(Xtrain_scaled, Ytrain)\n","\n","# let's print the error (1 - R^2) on training data\n","print(\"training error\", 1-NN.score(Xtrain_scaled, Ytrain))\n","\n","# let's print the error (1 - R^2) on validation data\n","print(\"validation error\", 1-NN.score(Xval_scaled, Yval))\n","\n","# # let's print the coefficients of the model for the input nodes (but not the bias)\n","# print(\"coeffs (weights from input layer to the hidden neuron):\\n\", NN.coefs_)\n","\n","# # let's print the coefficient for the bias (i.e., the bias)\n","# print(\"intercepts (weights of biases of input and hidden layers):\\n\", NN.intercepts_)"]},{"cell_type":"markdown","metadata":{"id":"oUTcpnKT9ltF"},"source":["Let's try 2 layers, 1 node each"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3BC3EYth9ltF","executionInfo":{"status":"ok","timestamp":1642527317059,"user_tz":-60,"elapsed":463,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}},"outputId":"a8894ad3-3bcf-45b2-ad95-6d32a8f7d3da"},"outputs":[{"output_type":"stream","name":"stdout","text":["training error 0.2709866028192074\n","validation error 0.28542758123222955\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"]}],"source":["# let's build a NN with 2 hidden layers, 1 node each\n","\n","# let's define the model\n","NN = MLPRegressor(hidden_layer_sizes=(1,1), random_state=numero_di_matricola, solver=\"lbfgs\")\n","\n","# let's learn the model on training data\n","NN.fit(Xtrain_scaled, Ytrain)\n","\n","# let's print the error (1 - R^2) on training data\n","print(\"training error\", 1-NN.score(Xtrain_scaled, Ytrain))\n","\n","# let's print the error (1 - R^2) on validation data\n","print(\"validation error\", 1-NN.score(Xval_scaled, Yval))\n","\n","# # let's print the coefficients of the model for the input nodes (but not the bias)\n","# print(\"coeffs (weights from input layer to the hidden neuron):\\n\", NN.coefs_)\n","\n","# # let's print the coefficient for the bias (i.e., the bias)\n","# print(\"intercepts (weights of biases of input and hidden layers):\\n\", NN.intercepts_)"]},{"cell_type":"markdown","metadata":{"id":"qEgw6b8E9ltF"},"source":["Let's try 2 layers, 2 nodes each"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UHZvZsjr9ltG","executionInfo":{"status":"ok","timestamp":1642527317755,"user_tz":-60,"elapsed":698,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}},"outputId":"d091e6e2-ebe7-49c9-a348-119477cf6837"},"outputs":[{"output_type":"stream","name":"stdout","text":["training error 0.24579106567172326\n","validation error 0.2516283049275865\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"]}],"source":["# let's build a NN with 2 layers, 2 nodes each\n","\n","# let's define the model\n","NN = MLPRegressor(hidden_layer_sizes=(2,2), random_state=numero_di_matricola, solver=\"lbfgs\")\n","\n","# let's learn the model on training data\n","NN.fit(Xtrain_scaled, Ytrain)\n","\n","# let's print the error (1 - R^2) on training data\n","print(\"training error\", 1-NN.score(Xtrain_scaled, Ytrain))\n","\n","# let's print the error (1 - R^2) on validation data\n","print(\"validation error\", 1-NN.score(Xval_scaled, Yval))\n","\n","# # let's print the coefficients of the model for the input nodes (but not the bias)\n","# print(\"coeffs (weights from input layer to the hidden neuron):\\n\", NN.coefs_)\n","\n","# # let's print the coefficient for the bias (i.e., the bias)\n","# print(\"intercepts (weights of biases of input and hidden layers):\\n\", NN.intercepts_)"]},{"cell_type":"markdown","metadata":{"id":"P7lxuXSO9ltG"},"source":["Let's try 2 layers, 10 nodes each"]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ez0-63Hn9ltG","executionInfo":{"status":"ok","timestamp":1642527320386,"user_tz":-60,"elapsed":2632,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}},"outputId":"8ea6b99d-8c44-4828-ca21-9e44db244c8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["training error 0.1490405350069579\n","validation error 0.16497647245517488\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"]}],"source":["# let's build a NN with 2 layers, 10 nodes each\n","\n","# let's define the model\n","NN = MLPRegressor(hidden_layer_sizes=(10,10), random_state=numero_di_matricola, solver=\"lbfgs\")\n","\n","# let's learn the model on training data\n","NN.fit(Xtrain_scaled, Ytrain)\n","\n","# let's print the error (1 - R^2) on training data\n","print(\"training error\", 1-NN.score(Xtrain_scaled, Ytrain))\n","\n","# let's print the error (1 - R^2) on validation data\n","print(\"validation error\", 1-NN.score(Xval_scaled, Yval))\n","\n","# # let's print the coefficients of the model for the input nodes (but not the bias)\n","# print(\"coeffs (weights from input layer to the hidden neuron):\\n\", NN.coefs_)\n","\n","# # let's print the coefficient for the bias (i.e., the bias)\n","# print(\"intercepts (weights of biases of input and hidden layers):\\n\", NN.intercepts_)\n"]},{"cell_type":"markdown","metadata":{"id":"1SeYC1Xv9ltH"},"source":["Let's try 2 layers, 100 nodes each"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fp7iU1fY9ltH","executionInfo":{"status":"ok","timestamp":1642527352148,"user_tz":-60,"elapsed":31764,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}},"outputId":"337c3ce5-1481-404e-94e7-be3ddb6eb623"},"outputs":[{"output_type":"stream","name":"stdout","text":["training error 0.06478762206269884\n","validation error 0.11640398000193142\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"]}],"source":["# let's build a NN with 2 layers, 100 nodes each\n","\n","# let's define the model\n","NN = MLPRegressor(hidden_layer_sizes=(100,100), random_state=numero_di_matricola, solver=\"lbfgs\")\n","\n","# let's learn the model on training data\n","NN.fit(Xtrain_scaled, Ytrain)\n","\n","# let's print the error (1 - R^2) on training data\n","print(\"training error\", 1-NN.score(Xtrain_scaled, Ytrain))\n","\n","# let's print the error (1 - R^2) on validation data\n","print(\"validation error\", 1-NN.score(Xval_scaled, Yval))\n","\n","# # let's print the coefficients of the model for the input nodes (but not the bias)\n","# print(\"coeffs (weights from input layer to the hidden neuron):\\n\", NN.coefs_)\n","\n","# # let's print the coefficient for the bias (i.e., the bias)\n","# print(\"intercepts (weights of biases of input and hidden layers):\\n\", NN.intercepts_)\n"]},{"cell_type":"markdown","metadata":{"id":"GLmkp9C79ltI"},"source":["So it seems that 1 layer (and default number of iterations) works best for this dataset. Let's try 5-fold cross-validation with number of nodes in the hidden layer between 1 and 20.\n","Note that we use train and validation data together, since we are doing cross-validation."]},{"cell_type":"code","execution_count":64,"metadata":{"id":"kBO3X4XE9ltI","executionInfo":{"status":"ok","timestamp":1642527352149,"user_tz":-60,"elapsed":18,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}}},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","# COMPLETE\n"]},{"cell_type":"markdown","metadata":{"id":"kdmCNSIu9ltI"},"source":["Now let's check what is the best parameter, and compare the best NNs with the linear model (learned on train and validation) on test data."]},{"cell_type":"code","execution_count":65,"metadata":{"id":"PJ9hT_hu9ltI","executionInfo":{"status":"ok","timestamp":1642527352150,"user_tz":-60,"elapsed":18,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}}},"outputs":[],"source":["# let's print the best model according to grid search\n","# COMPLETE\n","\n","# let's print the error 1-R^2 for the best model\n","# COMPLETE\n"]},{"cell_type":"markdown","metadata":{"id":"nxNbbOp_9ltJ"},"source":["Let compare the error of the best NN on train and validation and on test data."]},{"cell_type":"code","execution_count":66,"metadata":{"id":"i5yGw4wu9ltJ","executionInfo":{"status":"ok","timestamp":1642527352150,"user_tz":-60,"elapsed":17,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}}},"outputs":[],"source":["# COMPLETE\n"]},{"cell_type":"markdown","metadata":{"id":"w-aCD5Dk9ltJ"},"source":["Now let's learn the linear model on train and validation, and get error (1-R^2) on train and validation and on test data."]},{"cell_type":"code","execution_count":67,"metadata":{"id":"hjqxJJff9ltL","executionInfo":{"status":"ok","timestamp":1642527352151,"user_tz":-60,"elapsed":18,"user":{"displayName":"Stefano Binotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicDr4IKFD48BATG3l6K7No-9YUjTbR6Nb8PhG3uQ=s64","userId":"11249391583011541557"}}},"outputs":[],"source":["# COMPLETE\n"]},{"cell_type":"markdown","metadata":{"id":"MdYYNZIw9ltL"},"source":["Note: MLPRegressor has several other parameters!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"colab":{"name":"Lab4_part2.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}